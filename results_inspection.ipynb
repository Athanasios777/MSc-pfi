{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import seaborn as sns; sns.set()  # for plot styling\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "import xarray as xr\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.patches as mpatches\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import random\n",
    "from functions1 import *\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from torchdataloaders import TestDataset,get_variable_options, TestDatasetPermute\n",
    "from utils1 import CHARTS, SIC_LOOKUP, SOD_LOOKUP, FLOE_LOOKUP, SCENE_VARIABLES, colour_str\n",
    "\n",
    "\n",
    "os.environ['AI4ARCTIC_DATA'] = '/work3/s222651/data/data/dataV2/'\n",
    "os.environ['AI4ARCTIC_ENV'] = '/work3/s222651/'\n",
    "train_options = {\n",
    "    # -- Training options -- #\n",
    "    'path_to_processed_data': os.environ['AI4ARCTIC_DATA'], \n",
    "    'test_files': '/work3/s222651/test/data/testdata/', \n",
    "    'path_to_env': os.environ['AI4ARCTIC_ENV'],  \n",
    "    'lr': 0.0001,  # Optimizer learning rate.\n",
    "    'epochs': 70,  # Number of epochs before training stop.\n",
    "    'epoch_len': 500,  # Number of batches for each epoch.\n",
    "    'patch_size': 256,  # Size of patches sampled. Used for both Width and Height.\n",
    "    'batch_size': 8,  # Number of patches for each batch.\n",
    "    'loader_upsampling': 'nearest',  # How to upscale low resolution variables to high resolution.\n",
    "    \n",
    "    # -- Data prepraration lookups and metrics.\n",
    "    'train_variables': SCENE_VARIABLES,  # Contains the relevant variables in the scenes.\n",
    "    'charts': CHARTS,  # Charts to train on.\n",
    "    'n_classes': {  # number of total classes in the reference charts, including the mask.\n",
    "        'SIC': SIC_LOOKUP['n_classes'],\n",
    "        'SOD': SOD_LOOKUP['n_classes'],\n",
    "        'FLOE': FLOE_LOOKUP['n_classes']\n",
    "    },\n",
    "    'pixel_spacing': 80,  # SAR pixel spacing. 80 for the ready-to-train AI4Arctic Challenge dataset.\n",
    "    'train_fill_value': 0,  # Mask value for SAR training data.\n",
    "    'class_fill_values': {  # Mask value for class/reference data.\n",
    "        'SIC': SIC_LOOKUP['mask'],\n",
    "        'SOD': SOD_LOOKUP['mask'],\n",
    "        'FLOE': FLOE_LOOKUP['mask'],\n",
    "    },\n",
    "    \n",
    "    # -- Validation options -- #\n",
    "    'chart_metric': {  # Metric functions for each ice parameter and the associated weight.\n",
    "        #'SIC': {\n",
    "        #    'func': r2_metric,\n",
    "        #   'weight': 2,\n",
    "        #},\n",
    "        'SOD': {\n",
    "            'func': f1_metric,\n",
    "        #    'weight': 2,\n",
    "        },\n",
    "        #'FLOE': {\n",
    "        #    'func': f1_metric,\n",
    "        #    'weight': 1,\n",
    "        \n",
    "    },\n",
    "    'num_val_scenes': 20,  # Number of scenes randomly sampled from train_list to use in validation.\n",
    "    \n",
    "    # -- GPU/cuda options -- #\n",
    "    'gpu_id': 0,  # Index of GPU. In case of multiple GPUs.\n",
    "    'num_workers': 8,  # Number of parallel processes to fetch data.\n",
    "    'num_workers_val': 1,  # Number of parallel processes during validation.\n",
    "    \n",
    "    # -- U-Net Options -- #\n",
    "    'unet_conv_filters': [16, 32, 64, 64],  # Number of filters in the U-Net.\n",
    "    'conv_kernel_size': (3, 3),  # Size of convolutional kernels.\n",
    "    'conv_stride_rate': (1, 1),  # Stride rate of convolutional kernels.\n",
    "    'conv_dilation_rate': (1, 1),  # Dilation rate of convolutional kernels.\n",
    "    'conv_padding': (1, 1),  # Number of padded pixels in convolutional layers.\n",
    "    'conv_padding_style': 'zeros',  # Style of padding.\n",
    "}\n",
    "# Get options for variables, amsrenv grid, cropping and upsampling.\n",
    "get_variable_options = get_variable_options(train_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inspection of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open nc file \n",
    "#ds = xr.open_dataset('C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\dataV2\\\\20211220T102441_dmi_prep.nc') #this is ready to train version\n",
    "ds = xr.open_dataset('C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\testdata\\\\20210506T075557_dmi_prep.nc') \n",
    "#ds = xr.open_dataset('C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\raw\\\\S1A_EW_GRDM_1SDH_20190406T102029_20190406T102058_026668_02FE1F_C66E_icechart_cis_SGRDINFLD_20190406T1019Z_pl_a.nc') # this is the original version\n",
    "HH = ds['nersc_sar_primary']\n",
    "HV = ds['nersc_sar_secondary']\n",
    "bt_6_9h = ds['btemp_6_9h']\n",
    "bt_6_9v = ds['btemp_6_9v']\n",
    "bt_7_3v = ds['btemp_7_3v']\n",
    "bt_7_3h = ds['btemp_7_3h']\n",
    "bt_10_7h = ds['btemp_10_7h']\n",
    "bt_10_7v = ds['btemp_10_7v']\n",
    "bt_18_7h = ds['btemp_18_7h']\n",
    "bt_18_7v = ds['btemp_18_7v']    \n",
    "bt_23_8h = ds['btemp_23_8h']\n",
    "bt_23_8v = ds['btemp_23_8v']\n",
    "bt_36_5h = ds['btemp_36_5h']\n",
    "bt_36_5v = ds['btemp_36_5v']\n",
    "bt_89_0h = ds['btemp_89_0h']\n",
    "bt_89_0v = ds['btemp_89_0v']\n",
    "#sodlabel = ds['SOD']\n",
    "\n",
    "\n",
    "# plot the HH and SOD label in a figure with their colorbars \n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 5))\n",
    "im0 = ax[0].imshow(HH, cmap='gray')\n",
    "ax[0].set_title('SAR - HV')\n",
    "ax[0].grid(False)\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "divider = make_axes_locatable(ax[0])\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "cbar0 = plt.colorbar(im0, cax=cax)\n",
    "cbar0.ax.set_title(r'$\\sigma^0 \\ [dB]$', loc='center', fontsize=10)\n",
    "\n",
    "im1 = ax[1].imshow(bt_36_5v, cmap='viridis')\n",
    "ax[1].set_title('36.5 GHz - V polarization')\n",
    "ax[1].grid(False)\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "divider = make_axes_locatable(ax[1])\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "cbar1 = plt.colorbar(im1, cax=cax)\n",
    "cbar1.ax.set_title('Bt', loc='center', fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training nc files to count class occurences\n",
    "\n",
    "directory = 'C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\dataV2\\\\'\n",
    "\n",
    "# List to store the counts of each class\n",
    "class_counts = np.zeros(6)  \n",
    "\n",
    "# Total valid pixels counter (excluding 255)\n",
    "total_valid_pixels = 0\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.nc'):\n",
    "        # Load the dataset\n",
    "        ds = xr.open_dataset(os.path.join(directory, filename))\n",
    "        \n",
    "        # Extract the 'SOD' variable\n",
    "        sod = ds['SOD'].values\n",
    "        \n",
    "        # Count each class ignoring 255\n",
    "        unique, counts = np.unique(sod[sod != 255], return_counts=True)\n",
    "        #unique, counts = np.unique(sod, return_counts=True)\n",
    "        \n",
    "        # Update class counts and total valid pixels\n",
    "        for i, val in enumerate(unique):\n",
    "            if val in range(6):  # Ensure class value is within 0-5\n",
    "                class_counts[int(val)] += counts[i]\n",
    "        total_valid_pixels += np.sum(counts)\n",
    "\n",
    "        # Close the dataset to free resources\n",
    "        ds.close()\n",
    "\n",
    "percentages = (class_counts / total_valid_pixels) * 100\n",
    "\n",
    "for i, pct in enumerate(percentages):\n",
    "    print(f\"Class {i}: {pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of the class occurences\n",
    "classes = ['Open-water', 'New ice', 'Young ice ', 'Thin FYI', 'Thick FYI', 'Old ice']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(classes, percentages)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Percentage [%]')\n",
    "plt.title('SOD class distribution')\n",
    "for i, pct in enumerate(percentages):\n",
    "    plt.text(i, pct, f\"{pct:.2f}%\", ha='center', va='bottom', fontsize=9)\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "bt = bt\n",
    "flattened_bt = bt.flatten()\n",
    "perm_indices = torch.randperm(flattened_bt.shape[0])\n",
    "bt_permuted = flattened_bt[perm_indices].reshape(bt.shape)\n",
    "\n",
    "# Plotting the original and permuted 36.5 GHz V band\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Original data\n",
    "im0 = ax[0].imshow(bt_36_5h, cmap='viridis')\n",
    "ax[0].set_title('36.5 GHz - V polarization')\n",
    "ax[0].grid(False)\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "divider = make_axes_locatable(ax[0])\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "cbar0 = plt.colorbar(im0, cax=cax)\n",
    "cbar0.ax.set_title('Bt', loc='center', fontsize=10)\n",
    "\n",
    "# Permuted data\n",
    "im1 = ax[1].imshow(bt_permuted, cmap='viridis')\n",
    "ax[1].set_title('36.5 GHz - V polarization - Permuted')\n",
    "ax[1].grid(False)\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "divider = make_axes_locatable(ax[1])\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "cbar1 = plt.colorbar(im1, cax=cax)\n",
    "cbar1.ax.set_title('Bt', loc='center', fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate the VALIDATION files\n",
    "#val_dir = 'C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\validationdata\\\\'\n",
    "\n",
    "test_dir = 'C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\testdata\\\\'\n",
    "\n",
    "# import nc files \n",
    "# validation data\n",
    "\n",
    "testfile1 = test_dir + '20180124T194759_dmi_prep.nc'\n",
    "testfile2 = test_dir + '20180623T114935_cis_prep.nc'\n",
    "testfile3 = test_dir + '20180707T113313_cis_prep.nc'\n",
    "testfile4 = test_dir + '20180716T110418_cis_prep.nc'\n",
    "testfile5 = test_dir + '20180903T123331_cis_prep.nc'\n",
    "testfile6 = test_dir + '20180917T121813_cis_prep.nc'\n",
    "testfile7 = test_dir + '20190406T102029_cis_prep.nc'\n",
    "testfile8 = test_dir + '20190810T110422_dmi_prep.nc'\n",
    "testfile9 = test_dir + '20191011T131651_cis_prep.nc'\n",
    "testfile10 = test_dir + '20200217T102731_cis_prep.nc'\n",
    "testfile11 = test_dir + '20200319T101935_cis_prep.nc'\n",
    "testfile12 = test_dir + '20200701T114012_cis_prep.nc'\n",
    "testfile13 = test_dir + '20200719T123046_cis_prep.nc'\n",
    "testfile14 = test_dir + '20201013T080448_dmi_prep.nc'\n",
    "testfile15 = test_dir + '20210328T202742_dmi_prep.nc'\n",
    "testfile16 = test_dir + '20210410T201933_dmi_prep.nc'\n",
    "testfile17 = test_dir + '20210430T205436_dmi_prep.nc'\n",
    "testfile18 = test_dir + '20210506T075557_dmi_prep.nc'\n",
    "testfile19 = test_dir + '20210512T214149_cis_prep.nc'\n",
    "testfile20 = test_dir + '20211212T211242_dmi_prep.nc'\n",
    "\n",
    "testds1 = xr.open_dataset(testfile1)\n",
    "testds2 = xr.open_dataset(testfile2)\n",
    "testds3 = xr.open_dataset(testfile3)\n",
    "testds4 = xr.open_dataset(testfile4)\n",
    "testds5 = xr.open_dataset(testfile5)\n",
    "testds6 = xr.open_dataset(testfile6)\n",
    "testds7 = xr.open_dataset(testfile7)\n",
    "testds8 = xr.open_dataset(testfile8)\n",
    "testds9 = xr.open_dataset(testfile9)\n",
    "testds10 = xr.open_dataset(testfile10)\n",
    "testds11 = xr.open_dataset(testfile11)\n",
    "testds12 = xr.open_dataset(testfile12)\n",
    "testds13 = xr.open_dataset(testfile13)\n",
    "testds14 = xr.open_dataset(testfile14)\n",
    "testds15 = xr.open_dataset(testfile15)\n",
    "testds16 = xr.open_dataset(testfile16)\n",
    "testds17 = xr.open_dataset(testfile17)\n",
    "testds18 = xr.open_dataset(testfile18)\n",
    "testds19 = xr.open_dataset(testfile19)\n",
    "testds20 = xr.open_dataset(testfile20)\n",
    "\n",
    "# plot the longitude and latitude of the test data in the Arctic region in one plot (polar stereographic projection)\n",
    "fig, ax = plt.subplots(subplot_kw={'projection': ccrs.NorthPolarStereo()}, figsize=(7, 7))\n",
    "ax.set_extent([-150, 150, 50, 90], ccrs.PlateCarree())\n",
    "ax.add_feature(cfeature.LAND,facecolor='lightgray', edgecolor='black')\n",
    "ax.add_feature(cfeature.COASTLINE, linewidth=0.8)\n",
    "ax.add_feature(cfeature.BORDERS, linewidth=0.5)\n",
    "ax.gridlines()\n",
    "ax.set_title('Test scenes')\n",
    "ax.plot(testds1['sar_grid2d_longitude'].values, testds1['sar_grid2d_latitude'].values, color = 'red', linewidth = 0.5, transform=ccrs.PlateCarree())\n",
    "ax.plot(testds2['sar_grid2d_longitude'].values, testds2['sar_grid2d_latitude'].values, color = 'red', linewidth = 0.5, transform=ccrs.PlateCarree())\n",
    "ax.plot(testds3['sar_grid2d_longitude'].values, testds3['sar_grid2d_latitude'].values, color = 'red', linewidth = 0.5, transform=ccrs.PlateCarree())\n",
    "ax.plot(testds4['sar_grid2d_longitude'].values, testds4['sar_grid2d_latitude'].values, color = 'red', linewidth = 0.5, transform=ccrs.PlateCarree())\n",
    "ax.plot(testds5['sar_grid2d_longitude'].values, testds5['sar_grid2d_latitude'].values, color = 'red', linewidth = 0.5, transform=ccrs.PlateCarree())\n",
    "ax.plot(testds6['sar_grid2d_longitude'].values, testds6['sar_grid2d_latitude'].values, color = 'red', linewidth = 0.5, transform=ccrs.PlateCarree())\n",
    "ax.plot(testds7['sar_grid2d_longitude'].values, testds7['sar_grid2d_latitude'].values, color = 'red', linewidth = 0.5, transform=ccrs.PlateCarree())\n",
    "ax.plot(testds8['sar_grid2d_longitude'].values, testds8['sar_grid2d_latitude'].values, color = 'red', linewidth = 0.5, transform=ccrs.PlateCarree())\n",
    "ax.plot(testds9['sar_grid2d_longitude'].values, testds9['sar_grid2d_latitude'].values, color = 'red', linewidth = 0.5, transform=ccrs.PlateCarree())\n",
    "ax.plot(testds10['sar_grid2d_longitude'].values, testds10['sar_grid2d_latitude'].values, color = 'red', linewidth = 0.5, transform=ccrs.PlateCarree())\n",
    "ax.plot(testds11['sar_grid2d_longitude'].values, testds11['sar_grid2d_latitude'].values, color = 'red', linewidth = 0.5, transform=ccrs.PlateCarree())\n",
    "ax.plot(testds12['sar_grid2d_longitude'].values, testds12['sar_grid2d_latitude'].values, color = 'red', linewidth = 0.5, transform=ccrs.PlateCarree())\n",
    "ax.plot(testds13['sar_grid2d_longitude'].values, testds13['sar_grid2d_latitude'].values, color = 'red', linewidth = 0.5, transform=ccrs.PlateCarree())\n",
    "ax.plot(testds14['sar_grid2d_longitude'].values, testds14['sar_grid2d_latitude'].values, color = 'red', linewidth = 0.5, transform=ccrs.PlateCarree())\n",
    "ax.plot(testds15['sar_grid2d_longitude'].values, testds15['sar_grid2d_latitude'].values, color = 'red', linewidth = 0.5, transform=ccrs.PlateCarree())\n",
    "ax.plot(testds16['sar_grid2d_longitude'].values, testds16['sar_grid2d_latitude'].values, color = 'red', linewidth = 0.5, transform=ccrs.PlateCarree())\n",
    "ax.plot(testds17['sar_grid2d_longitude'].values, testds17['sar_grid2d_latitude'].values, color = 'red', linewidth = 0.5, transform=ccrs.PlateCarree())\n",
    "ax.plot(testds18['sar_grid2d_longitude'].values, testds18['sar_grid2d_latitude'].values, color = 'red', linewidth = 0.5, transform=ccrs.PlateCarree())\n",
    "ax.plot(testds19['sar_grid2d_longitude'].values, testds19['sar_grid2d_latitude'].values, color = 'red', linewidth = 0.5, transform=ccrs.PlateCarree())\n",
    "ax.plot(testds20['sar_grid2d_longitude'].values, testds20['sar_grid2d_latitude'].values, color = 'red', linewidth = 0.5, transform=ccrs.PlateCarree())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 70 epochs and 90 epochs with all channels - performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model70 = 'C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\test_results\\\\inference_70_epochs_16channels/best_model.pth'\n",
    "#model90 = 'C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\test_results\\\\inference_90_epochs_16channels/best_model.pth'\n",
    "\n",
    "checkpoint70 = torch.load(model70, map_location='cpu')\n",
    "#checkpoint90 = torch.load(model90, map_location='cpu')\n",
    "\n",
    "\n",
    "best_F1_score70 = checkpoint70['best_score']\n",
    "#best_F1_score90 = checkpoint90['best_score']\n",
    "\n",
    "print(f\"best F1 score (70 epochs and all channels): {best_F1_score70} %\")\n",
    "#print(f\"best F1 score (90 epochs and all channels): {best_F1_score90} %\")\n",
    "\n",
    "# plot the two confusion matrices side by side\n",
    "cm70 = np.load('C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\test_results\\\\inference_70_epochs_16channels//confusion_matrix.npy')\n",
    "cm_percentage70 = cm70 / cm70.sum(axis=1, keepdims=True) * 100\n",
    "\n",
    "#cm90 = np.load('C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\test_results\\\\inference_90_epochs_16channels//confusion_matrix.npy')\n",
    "#cm_percentage90 = cm90 / cm90.sum(axis=1, keepdims=True) * 100\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "sns.heatmap(cm_percentage70, annot=True, fmt=\".1f\", cmap='YlGnBu', cbar_kws={'label': 'Percentage of Predictions [%]'}, ax=ax[0])\n",
    "ax[0].set_xlabel('Predicted')\n",
    "ax[0].set_ylabel('True')\n",
    "ax[0].set_title('70 epochs all channels')\n",
    "\n",
    "# sns.heatmap(cm_percentage90, annot=True, fmt=\".1f\", cmap='YlGnBu', cbar_kws={'label': 'Percentage of Predictions [%]'}, ax=ax[1])\n",
    "# ax[1].set_xlabel('Predicted')\n",
    "# ax[1].set_ylabel('True')\n",
    "# ax[1].set_title('90 epochs all channels')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the predictions and reference to do plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference\n",
    "\n",
    "pathref = 'C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\testreference\\\\'\n",
    "scene_name = '20201013T080448_dmi_prep_reference.nc'\n",
    "\n",
    "ref = xr.open_dataset(pathref + scene_name)\n",
    "ref_sod = ref['SOD']\n",
    "print(ref_sod.shape)\n",
    "\n",
    "# replace the 255 values in the sod variable with nan\n",
    "ref_sod = ref_sod.where(ref_sod != 255, np.nan)\n",
    "\n",
    "\n",
    "path = 'C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\test_results\\\\inference_70_epochs_16channels\\\\'\n",
    "scene_name = '20201013T080448_dmi_prep'\n",
    "\n",
    "# load npz file \n",
    "pred = np.load(path + scene_name + '_data.npz')\n",
    "\n",
    "pred_sod = pred['SOD_pred']\n",
    "pred_masks = pred['test_mask']\n",
    "\n",
    "print(pred_sod.shape)\n",
    "\n",
    "boundaries = np.arange(-0.5, 6, 1)\n",
    "cmap = plt.cm.get_cmap('viridis', len(boundaries) - 1)\n",
    "norm = BoundaryNorm(boundaries, cmap.N)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 8))\n",
    "# plot SOD prediction vs reference\n",
    "im_pred = axs[0].imshow(pred_sod[...], cmap=cmap, norm=norm)\n",
    "axs[0].set_title('Prediction ')\n",
    "axs[0].grid(False)\n",
    "\n",
    "for spine in axs[0].spines.values():\n",
    "    spine.set_edgecolor('black')\n",
    "    spine.set_linewidth(1)\n",
    "\n",
    "im_ref = axs[1].imshow(ref_sod[...], cmap=cmap, norm=norm)\n",
    "axs[1].set_title('Truth')\n",
    "axs[1].grid(False)\n",
    "\n",
    "for spine in axs[1].spines.values():\n",
    "    spine.set_edgecolor('black')\n",
    "    spine.set_linewidth(1)\n",
    "\n",
    "cbar = plt.colorbar(im_pred, ax=axs, orientation = 'horizontal', fraction = 0.08, pad = 0.07, ticks = np.arange(0,len(boundaries)-1))\n",
    "\n",
    "class_labels = ['Open water', 'New ice', 'Young ice', 'Thin FYI', 'Thick FYI', 'Multiyear ice']\n",
    "cbar.set_ticklabels(class_labels)\n",
    "cbar.set_label('Stage of Development')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = 'C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\test_results\\\\inference_70_epochs_16channels/best_model.pth'\n",
    "model2 = 'C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\test_results\\\\inference_70_epochs_16channelsNEW/best_model.pth'\n",
    "model3 = 'C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\test_results\\\\inference_70_epochs_16channelsNEWNEW/best_model.pth'\n",
    "model4 = 'C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\test_results\\\\model4/best_model.pth'\n",
    "model5 = 'C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\test_results\\\\model5/best_model.pth'\n",
    "model6 = 'C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\test_results\\\\model6/best_model.pth'\n",
    "model7 = 'C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\test_results\\\\model7/best_model.pth'\n",
    "model8 = 'C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\test_results\\\\model8/best_model.pth'\n",
    "model9 = 'C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\test_results\\\\model9/best_model.pth'\n",
    "model10 = 'C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\test_results\\\\model10/best_model.pth'\n",
    "##\n",
    "checkpoint1 = torch.load(model1, map_location='cpu')\n",
    "checkpoint2 = torch.load(model2, map_location='cpu')\n",
    "checkpoint3 = torch.load(model3, map_location='cpu')\n",
    "checkpoint4 = torch.load(model4, map_location='cpu')\n",
    "checkpoint5 = torch.load(model5, map_location='cpu')\n",
    "checkpoint6 = torch.load(model6, map_location='cpu')\n",
    "checkpoint7 = torch.load(model7, map_location='cpu')\n",
    "checkpoint8 = torch.load(model8, map_location='cpu')\n",
    "checkpoint9 = torch.load(model9, map_location='cpu')\n",
    "checkpoint10 = torch.load(model10, map_location='cpu')\n",
    "##\n",
    "best_F1_score1 = checkpoint1['best_score']\n",
    "best_F1_score2 = checkpoint2['best_score']\n",
    "best_F1_score3 = checkpoint3['best_score']\n",
    "best_F1_score4 = checkpoint4['best_score']\n",
    "best_F1_score5 = checkpoint5['best_score']\n",
    "best_F1_score6 = checkpoint6['best_score']\n",
    "best_F1_score7 = checkpoint7['best_score']\n",
    "best_F1_score8 = checkpoint8['best_score']\n",
    "best_F1_score9 = checkpoint9['best_score']\n",
    "best_F1_score10 = checkpoint10['best_score']\n",
    "##\n",
    "print(f\"best F1 score (model1): {best_F1_score1} %\")\n",
    "print(f\"best F1 score (model2): {best_F1_score2} %\")\n",
    "print(f\"best F1 score (model3): {best_F1_score3} %\")\n",
    "print(f\"best F1 score (model4): {best_F1_score4} %\")\n",
    "print(f\"best F1 score (model5): {best_F1_score5} %\")\n",
    "print(f\"best F1 score (model6): {best_F1_score6} %\")\n",
    "print(f\"best F1 score (model7): {best_F1_score7} %\")\n",
    "print(f\"best F1 score (model8): {best_F1_score8} %\")\n",
    "print(f\"best F1 score (model9): {best_F1_score9} %\")\n",
    "print(f\"best F1 score (model10): {best_F1_score10} %\")\n",
    "\n",
    "# std of the F1 scores for the 10 models\n",
    "F1_scores = [best_F1_score1, best_F1_score2, best_F1_score3, best_F1_score4, best_F1_score5, best_F1_score6, best_F1_score7, best_F1_score8, best_F1_score9, best_F1_score10]\n",
    "\n",
    "std_F1_scores = np.std(F1_scores)\n",
    "mean_F1_scores = np.mean(F1_scores)\n",
    "\n",
    "# print mean and std with 2 decimal places\n",
    "print(f\"Mean F1 score: {mean_F1_scores:.2f} %\")\n",
    "print(f\"Std F1 score: {std_F1_scores:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = np.load('C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\test_results\\\\model4\\\\confusion_matrix.npy')\n",
    "cm_percentage = cm / cm.sum(axis=1, keepdims=True) * 100\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "sns.heatmap(cm_percentage, annot=True, fmt=\".1f\", cmap='YlGnBu', cbar_kws={'label': 'Percentage of Predictions [%]'}, ax=ax[0])\n",
    "ax[0].set_xlabel('Predicted')\n",
    "ax[0].set_ylabel('True')\n",
    "ax[0].set_title('Confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Excluding important channels 18.7 V / 23.8 H / 36.5 V & H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 1: excluding important channels 18.7 V / 23.8 H  / 36.5 V & H\n",
    "\n",
    "model_exp1 = 'C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\test_results\\\\exp1\\\\best_model.pth'\n",
    "checkpoint_exp1 = torch.load(model_exp1, map_location='cpu')\n",
    "best_F1_score_exp1 = checkpoint_exp1['best_score']\n",
    "print(f\"best F1 score (experiment 1): {best_F1_score_exp1} %\")\n",
    "\n",
    "# it is out out of the std deviation of the 10 models !\n",
    "\n",
    "\n",
    "cm = np.load('C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\test_results\\\\exp1\\\\confusion_matrix.npy')\n",
    "cm_percentage = cm / cm.sum(axis=1, keepdims=True) * 100\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "sns.heatmap(cm_percentage, annot=True, fmt=\".1f\", cmap='YlGnBu', cbar_kws={'label': 'Percentage of Predictions [%]'}, ax=ax[0])\n",
    "ax[0].set_xlabel('Predicted')\n",
    "ax[0].set_ylabel('True')\n",
    "ax[0].set_title('Confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference from the experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load the label\n",
    "\n",
    "ds = xr.open_dataset('C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\testreference\\\\20200319T101935_cis_prep_reference.nc') \n",
    "\n",
    "sod = ds['SOD']\n",
    "\n",
    "# whatever is 255, it is nan\n",
    "sod = sod.where(sod != 255)\n",
    "\n",
    "sod_flip = np.flipud(sod)\n",
    "\n",
    "\n",
    "for chart in train_options['charts']:\n",
    "\n",
    "    #plot the sod \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    im = ax.imshow(sod_flip, vmin=0, vmax = train_options['n_classes'][chart] -2, cmap ='jet', interpolation = 'nearest')\n",
    "    ax.set_title('SOD')\n",
    "    chart_cbar(ax=ax,n_classes=train_options['n_classes'][chart], chart=chart, cmap='jet')\n",
    "    # grid off\n",
    "    ax.grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Excluding very low and very high frequencies  6.9 / 7.3 / 89.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 2: excluding very low and very high frequencies 6.9 / 7.3 / 89.0 both polarizations\n",
    "model_exp2 = 'C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\test_results\\\\exp2/best_model.pth'\n",
    "checkpoint_exp2 = torch.load(model_exp2, map_location='cpu')\n",
    "best_F1_score_exp2 = checkpoint_exp2['best_score']\n",
    "print(f\"best F1 score (experiment 2): {best_F1_score_exp2} %\")\n",
    "\n",
    "# it is out out of the std deviation of the 10 models !\n",
    "\n",
    "\n",
    "cm = np.load('C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\test_results\\\\exp2\\\\confusion_matrix.npy')\n",
    "cm_percentage = cm / cm.sum(axis=1, keepdims=True) * 100\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "sns.heatmap(cm_percentage, annot=True, fmt=\".1f\", cmap='YlGnBu', cbar_kws={'label': 'Percentage of Predictions [%]'}, ax=ax[0])\n",
    "ax[0].set_xlabel('Predicted')\n",
    "ax[0].set_ylabel('True')\n",
    "ax[0].set_title('Confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Simulation of CIMR --  6.9 / 10.65 / 18.7 / 36.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 2: excluding very low and very high frequencies 6.9 / 7.3 / 89.0 both polarizations\n",
    "model_exp3 = 'C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\test_results\\\\cimr/best_model.pth'\n",
    "checkpoint_exp3 = torch.load(model_exp3, map_location='cpu')\n",
    "best_F1_score_exp3 = checkpoint_exp3['best_score']\n",
    "print(f\"best F1 score (experiment 3): {best_F1_score_exp3} %\")\n",
    "\n",
    "\n",
    "# print the model details\n",
    "#print(checkpoint_exp3)\n",
    "\n",
    "cm = np.load('C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\test_results\\\\cimr\\\\confusion_matrix.npy')\n",
    "cm_percentage = cm / cm.sum(axis=1, keepdims=True) * 100\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "sns.heatmap(cm_percentage, annot=True, fmt=\".1f\", cmap='YlGnBu', cbar_kws={'label': 'Percentage of Predictions [%]'}, ax=ax[0])\n",
    "ax[0].set_xlabel('Predicted')\n",
    "ax[0].set_ylabel('True')\n",
    "ax[0].set_title('Confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not officially implemented from now on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 - only H polarization  (10.65 H, 18.7 H, 23.5 H, 36.5 H )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_exp2_onlyH = 'C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\test_results\\\\exp2_onlyH\\\\best_model.pth'\n",
    "checkpoint_exp4 = torch.load(model_exp2_onlyH, map_location='cpu')\n",
    "best_F1_score_exp4 = checkpoint_exp4['best_score']\n",
    "print(f\"best F1 score (experiment 2 only H pol): {best_F1_score_exp4} %\")\n",
    "\n",
    "# it is out out of the std deviation of the 10 models !\n",
    "\n",
    "\n",
    "cm = np.load('C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\test_results\\\\exp2_onlyH\\\\confusion_matrix.npy')\n",
    "cm_percentage = cm / cm.sum(axis=1, keepdims=True) * 100\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "sns.heatmap(cm_percentage, annot=True, fmt=\".1f\", cmap='YlGnBu', cbar_kws={'label': 'Percentage of Predictions [%]'}, ax=ax[0])\n",
    "ax[0].set_xlabel('Predicted')\n",
    "ax[0].set_ylabel('True')\n",
    "ax[0].set_title('Confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 - only V polarization  (10.65 V, 18.7 V, 23.5 V, 36.5 V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_exp2_onlyV = 'C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\test_results\\\\exp2_onlyV\\\\best_model.pth'\n",
    "checkpoint_exp5 = torch.load(model_exp2_onlyV, map_location='cpu')\n",
    "best_F1_score_exp5 = checkpoint_exp5['best_score']\n",
    "print(f\"best F1 score (experiment 2 only v pol): {best_F1_score_exp5} %\")\n",
    "\n",
    "# it is out out of the std deviation of the 10 models !\n",
    "\n",
    "\n",
    "cm = np.load('C:\\\\Users\\\\HpRyzen7\\\\Desktop\\\\Thesis\\\\AI4Arctic\\\\test_results\\\\exp2_onlyV\\\\confusion_matrix.npy')\n",
    "cm_percentage = cm / cm.sum(axis=1, keepdims=True) * 100\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "sns.heatmap(cm_percentage, annot=True, fmt=\".1f\", cmap='YlGnBu', cbar_kws={'label': 'Percentage of Predictions [%]'}, ax=ax[0])\n",
    "ax[0].set_xlabel('Predicted')\n",
    "ax[0].set_ylabel('True')\n",
    "ax[0].set_title('Confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
